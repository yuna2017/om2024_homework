# 运维期末作业

BY 运维部 李文杰

## 摘要

本方案给出了一种面向企业用户友好的，本地部署Deepseek R1 70B的一种可能的解决方案，构建了一套本地知识库工作流，期望能够帮助企业利用AI大模型实现“增效”。

## 关键词

Deepseek LLM 异构计算 |RAG知识库|重排列|MCP server|数据库|国产之光|

## 引入

  **2022年8月9日**，美国正式签署《**芯片与科学法案**》，禁止向华出售高性能计算芯片，其中包括了民用的RTX5090等，以及面向AI训练的H100、Gaudi 3 AI等芯片，近期甚至要决定在禁华出售的芯片里安装定位系统并且可以远程关闭，来彻底禁止这些芯片在国内使用。因此，推进AI国产化的进程刻不容缓。

  目前已有的方案很多，甚至在华为昇腾社区已经正式上线Deepseek-R1、Deepseek-V3、Deepseek-V2、Janus-Pro ，支持一键获取 Deepseek 系列模型，支持昇腾硬件平台上开箱即用，推理快速部署。

  禁止使用对华禁售的产品的情况下，在硬件层面其实如果真要部署可用的Deepseek的选择并不多，核心计算集群主要就是**华为昇腾910B**，MTT S4000则疑似有点过于幽默了

  目前已经有企业开始使用AI大模型进行**“降本增效”**，但你要说增没增效你别管。目前的主要问题不是在于大模型本身的参数如何厉害，大模型的性能多强悍，而是在于如何真正的运用到生产中去，如何真正的让用户能够得心应手的使用大模型。

  优势就在于能够给企业用户一个可以落地实施使用的方案，能够将公司资料存放到本地，无需担心云存储导致的资料泄露问题，帮助企业**“增效”**

  部署上不选用Deepseek R1 671B，因为基本上没有企业有这样的需求，也没有几家公司能够负担的起这么大的成本，采用部署70B蒸馏版。

  这一世，我将夺回我的一切！

## 硬件物料清单

| 硬件名称 | 硬件数量 | 生产厂家 | 单价 | 总价 |
| -------- | -------- | -------- | ---- | ---- |
| 华为Atlas 800 9000 | 1    | 华为     | ？   | ？   |
| 三星DDR4-3200 RDIMM | 32 | 三星     | 150  | 4800 |
| 华为ES3600 NVMe SSD | 4 | 华为     | 899  | 3596 |
| 华为液冷供电模块 | 4 | 华为     | ？   | ？   |

价格面议，我没那个本事真去问（

## 技术规格表

| 产品名    | 型号                                           |
| --------- | ---------------------------------------------- |
| CPU       | 4 × HUAWEI Kunpeng 920 5250 (2.6GHz, 48 cores) |
| 内存      | 三星DDR4-3200 RDIMM                            |
| 硬盘      | 华为ES3600 NVMe SSD                            |
| NPU       | 8 × Ascend 910B 32GB                           |
| 供电/电源 | 华为液冷供电模块                               |
| 最大功耗  | 5000W                                          |
| 散热方式  | 风冷                                           |

相当简单低成本的系统。

之前有想过许多方案，例如能不能用性能稍微差一点，但是成本低的消费级显卡，像是RTX 2060，来替代专业的计算卡，后来核算下来完全不值当，且不说显存带宽低，就连性价比上还不如一张Ascend 910B3，还有就是摩尔线程的方案，MTT S4000，我还以为这张卡是万元级别的，结果一张要十一万，怪不得自从发布后就没声响了，纯纯的没活硬整说是。只能说华为成为主流方案还是有他的原因的。华！为！

## 软件架构

### AI推理框架

  AI推理框架没什么可选的，基本上就直接选用**华为MindIE 2.0.T3**，原生支持华为昇腾NPU优化的推理引擎。

  MindIE架构由多个组件组成，包括：

  **MindIE-Service**（服务化层）支持Triton/OpenAI等主流接口，还包括三个子组件：**MindIE-Server**、**MindIE-Client**、**MindIE-MS**；

  **MindIE-Torch**（模型适配层），针对PyTorch框架模型的推理加速插件。它提供简易的C++/Python接口，能够在做出极少代码改动的前提下完成模型迁移；

  **MindIE-RT**（运行时引擎），基于昇腾CANN实现计算图优化与整图下发，集成Transformer加速库ATB（你要是问我这一大串都是什么意思，我也不知道，反正感觉很厉害就是了）。

### 本地知识库

  传统的RAG知识库确实能够减少大模型的幻觉问题，但是它本身的问题也不少。本地知识库的基本原理就是将文件切片成多个文本块，然后使用嵌入模型对文本块进行向量化，将向量和对应的文本保存在向量库中。用户提问的问题也会被转换为向量，在数据库中进行纯粹数学计算，比对出相似的片段，从而给出答案。

  RAG系统中，大模型很大程度上只是起一个归纳总结的作用，好坏完全在于向量数据库。

  第一个问题是，RAG切片过于简单粗暴，当一个段落过长时，就按照固定的字数进行分块，不提AI，人都难以理解这种碎片化语言的上下文关系。

  第二个问题就是，由于检索的时候时进行纯粹的数学计算匹配，不代表本身的文字含义，就会导致检索不精准。现在一般采用重排列来提高搜索的精准度

  第三个问题在于，面对表格数据的时候，由于知识库只会返回部分的数据，所以大模型很难去对数据进行统计，现在一般用MCP server对接数据库来解决，基于上述论述，则可以设计一套本地知识库工作流：



  ![核心流程](https://github.com/SUMMMERRR/markdown/blob/master/mermaid-diagram-2025-05-28-141033.png?raw=true)

嵌入模型：Huawei SE-Embedding-Large-v2，1024维度，昇腾NPU量化版，支持FP16加速

重排列模型：bge-reranker-large昇腾优化版

MCP服务端：cline（开源免费）

数据库系统：MySQL

## 参考与引用

[完美运行DeepSeek-R1 671B：摩尔线程MTT S4000通过中国信通院AI芯片和大模型适配验证--快科技--科技改变未来](https://news.mydrivers.com/1/1046/1046150.htm)

[摩尔线程国产GPU快速实现DeepSeek部署与应用_腾讯新闻](https://news.qq.com/rain/a/20250204A05S8700)

[想象：用AI降本增效，现实：降本增笑🤣，怎么避免AI落地的天坑？_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV18Cj1zBEEx/?spm_id_from=333.1391.0.0&vd_source=b563cadbd2d7b0283e5b9a197f0e3a15)

[华为昇腾 910B GPU - fengjian1585 - 博客园](https://www.cnblogs.com/fengjian2016/p/18767859)

[【搭建AI大模型】本地私有化RAG知识库搭建—基于Ollama+AnythingLLM保姆级教程_ollama部署rerank模型-CSDN博客](https://blog.csdn.net/bugyinyin/article/details/145030140)

[使用 KTransformers 优化后，DeepSeek 的推理速度-一万网络](https://www.idc10000.net/news/content/3909.html)

[升级 CPU 对 DeepSeek 速度提升的影响-一万网络](https://www.idc10000.net/news/content/3908.html)

[MindIE Service整体介绍&快速上手 - 知乎](https://zhuanlan.zhihu.com/p/16051278956)

[CherryStudio搭建本地AI知识库，三大痛点与进阶方案。_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1NMoFYoEsb/?spm_id_from=333.337.search-card.all.click&vd_source=b563cadbd2d7b0283e5b9a197f0e3a15)

[MCP是啥？技术原理是什么？一个视频搞懂MCP的一切。Windows系统配置MCP，Cursor Cline使用MCP_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1AnQNYxEsy?spm_id_from=333.788.videopod.sections&vd_source=b563cadbd2d7b0283e5b9a197f0e3a15)

## 鸣谢与感悟

其实我想了很多，不知道为什么最后写出来就这么点东西，可能大脑知识容量太稀少了吧，稍微学一点就过载

特别感谢两个在B站评论区吵架的哥们，虽然可能有点污染简中社区，但是这俩人的对话确实给了我一些启发

感谢运维部另外一位成员，我和他从高中开始就是同班同学，我很难去形容这一路走来他带给我的帮助有多大

感谢茂业的一家牛肉火锅店，因为我和前面说的那位同学30号要去吃这家店

最后感谢正在审阅这篇作业的你，这篇我付出诸多心血的东西纯纯答辩，我找不到任何理由去阅读它，但你还是耐着性子读完了，并且甚至读到了最后一行，真的感谢。